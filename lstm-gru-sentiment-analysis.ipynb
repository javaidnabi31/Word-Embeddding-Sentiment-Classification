{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Sentiment Analysis IMDb Dataset (using LSTM, GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews = X_train + X_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews) \n",
    "\n",
    "# pad sequences\n",
    "max_length = 100 # try other options like mean",
    "\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens =  tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125602\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2678, 100)         12560200  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,573,001\n",
      "Trainable params: 12,573,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/25\n",
      " - 627s - loss: 0.5720 - acc: 0.7023 - val_loss: 0.4527 - val_acc: 0.7915\n",
      "Epoch 2/25\n",
      " - 528s - loss: 0.3987 - acc: 0.8283 - val_loss: 0.3977 - val_acc: 0.8248\n",
      "Epoch 3/25\n",
      " - 310s - loss: 0.3556 - acc: 0.8517 - val_loss: 0.3921 - val_acc: 0.8291\n",
      "Epoch 4/25\n",
      " - 314s - loss: 0.3321 - acc: 0.8627 - val_loss: 0.4128 - val_acc: 0.8163\n",
      "Epoch 5/25\n",
      " - 315s - loss: 0.3081 - acc: 0.8730 - val_loss: 0.3972 - val_acc: 0.8252\n",
      "Epoch 6/25\n",
      " - 318s - loss: 0.2893 - acc: 0.8848 - val_loss: 0.3822 - val_acc: 0.8337\n",
      "Epoch 7/25\n",
      " - 319s - loss: 0.2732 - acc: 0.8910 - val_loss: 0.3745 - val_acc: 0.8438\n",
      "Epoch 8/25\n",
      " - 331s - loss: 0.2539 - acc: 0.9012 - val_loss: 0.5966 - val_acc: 0.7738\n",
      "Epoch 9/25\n",
      " - 326s - loss: 0.2607 - acc: 0.8956 - val_loss: 0.4067 - val_acc: 0.8280\n",
      "Epoch 10/25\n",
      " - 337s - loss: 0.2308 - acc: 0.9115 - val_loss: 0.3723 - val_acc: 0.8512\n",
      "Epoch 11/25\n",
      " - 336s - loss: 0.2213 - acc: 0.9152 - val_loss: 0.3656 - val_acc: 0.8543\n",
      "Epoch 12/25\n",
      " - 338s - loss: 0.2030 - acc: 0.9226 - val_loss: 0.3690 - val_acc: 0.8578\n",
      "Epoch 13/25\n",
      " - 5430s - loss: 0.1858 - acc: 0.9301 - val_loss: 0.3775 - val_acc: 0.8576\n",
      "Epoch 14/25\n",
      " - 401s - loss: 0.1845 - acc: 0.9314 - val_loss: 0.3876 - val_acc: 0.8526\n",
      "Epoch 15/25\n",
      " - 443s - loss: 0.1811 - acc: 0.9309 - val_loss: 0.3913 - val_acc: 0.8505\n",
      "Epoch 16/25\n",
      " - 607s - loss: 0.1677 - acc: 0.9380 - val_loss: 0.3849 - val_acc: 0.8597\n",
      "Epoch 17/25\n",
      " - 427s - loss: 0.1611 - acc: 0.9406 - val_loss: 0.4061 - val_acc: 0.8578\n",
      "Epoch 18/25\n",
      " - 410s - loss: 0.1532 - acc: 0.9428 - val_loss: 0.3951 - val_acc: 0.8582\n",
      "Epoch 19/25\n",
      " - 401s - loss: 0.1498 - acc: 0.9454 - val_loss: 0.4200 - val_acc: 0.8516\n",
      "Epoch 20/25\n",
      " - 379s - loss: 0.1442 - acc: 0.9488 - val_loss: 0.4272 - val_acc: 0.8492\n",
      "Epoch 21/25\n",
      " - 387s - loss: 0.1382 - acc: 0.9504 - val_loss: 0.4096 - val_acc: 0.8628\n",
      "Epoch 22/25\n",
      " - 387s - loss: 0.1464 - acc: 0.9475 - val_loss: 0.4110 - val_acc: 0.8592\n",
      "Epoch 23/25\n",
      " - 378s - loss: 0.1375 - acc: 0.9505 - val_loss: 0.4268 - val_acc: 0.8606\n",
      "Epoch 24/25\n",
      " - 358s - loss: 0.1219 - acc: 0.9571 - val_loss: 0.4327 - val_acc: 0.8554\n",
      "Epoch 25/25\n",
      " - 341s - loss: 0.1202 - acc: 0.9581 - val_loss: 0.4494 - val_acc: 0.8538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ad8cc90208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "25000/25000 [==============================] - 102s 4ms/step\n",
      "Test score: 0.44936159363269806\n",
      "Test accuracy: 0.8537999999809265\n",
      "Accuracy: 85.38%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9916264 ],\n",
       "       [0.29879633],\n",
       "       [0.0971105 ],\n",
       "       [0.00360127],\n",
       "       [0.16327332],\n",
       "       [0.0138877 ],\n",
       "       [0.16362512],\n",
       "       [0.00134153]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "test_sample_2 = \"Good movie!\"\n",
    "test_sample_3 = \"Maybe I like this movie.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
    "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
    "test_sample_6 = \"Bad movie!\"\n",
    "test_sample_7 = \"Not a good movie!\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1360948] 1  Wrong prdiction\n",
      "[0.29261437] 1  Wrong prdiction\n",
      "[0.9970799] 1  Right prdiction\n",
      "[0.87384164] 1  Right prdiction\n",
      "[0.99331176] 1  Right prdiction\n",
      "[0.97243744] 1  Right prdiction\n",
      "[0.85155106] 1  Right prdiction\n",
      "[0.7417877] 1  Right prdiction\n",
      "[0.9908635] 1  Right prdiction\n",
      "[0.9956833] 1  Right prdiction\n"
     ]
    }
   ],
   "source": [
    "#let us check how the model predicts\n",
    "classes = model.predict(X_test_pad[:10], batch_size=128)\n",
    "for i in range (0,10):\n",
    "    if(classes[i] > 0.5 and y_test[i] == 1 or (classes[i] <= 0.5 and y_test[i] == 0)):\n",
    "        print( classes[i], y_test[i], \" Right prdiction\")\n",
    "    else :\n",
    "        print( classes[i], y_test[i], \" Wrong prdiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 168,353\n",
      "Trainable params: 168,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 100, input_length=max_words))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/25\n",
      " - 513s - loss: 0.1798 - acc: 0.9324 - val_loss: 0.4092 - val_acc: 0.8448\n",
      "Epoch 2/25\n",
      " - 556s - loss: 0.1890 - acc: 0.9283 - val_loss: 0.4166 - val_acc: 0.8473\n",
      "Epoch 3/25\n",
      " - 946s - loss: 0.1736 - acc: 0.9345 - val_loss: 0.4416 - val_acc: 0.8446\n",
      "Epoch 4/25\n",
      " - 651s - loss: 0.1599 - acc: 0.9400 - val_loss: 0.4562 - val_acc: 0.8464\n",
      "Epoch 5/25\n",
      " - 688s - loss: 0.1635 - acc: 0.9390 - val_loss: 0.4382 - val_acc: 0.8432\n",
      "Epoch 6/25\n",
      " - 1021s - loss: 0.1507 - acc: 0.9440 - val_loss: 0.4438 - val_acc: 0.8478\n",
      "Epoch 7/25\n",
      " - 518s - loss: 0.1392 - acc: 0.9476 - val_loss: 0.4582 - val_acc: 0.8507\n",
      "Epoch 8/25\n",
      " - 516s - loss: 0.1573 - acc: 0.9391 - val_loss: 0.4841 - val_acc: 0.8454\n",
      "Epoch 9/25\n",
      " - 785s - loss: 0.1377 - acc: 0.9500 - val_loss: 0.4811 - val_acc: 0.8469\n",
      "Epoch 10/25\n",
      " - 1003s - loss: 0.1353 - acc: 0.9508 - val_loss: 0.4742 - val_acc: 0.8494\n",
      "Epoch 11/25\n",
      " - 630s - loss: 0.1753 - acc: 0.9315 - val_loss: 0.4758 - val_acc: 0.8385\n",
      "Epoch 12/25\n",
      " - 414s - loss: 0.1514 - acc: 0.9414 - val_loss: 0.4896 - val_acc: 0.8444\n",
      "Epoch 13/25\n",
      " - 409s - loss: 0.1239 - acc: 0.9551 - val_loss: 0.5066 - val_acc: 0.8482\n",
      "Epoch 14/25\n",
      " - 403s - loss: 0.1194 - acc: 0.9564 - val_loss: 0.5132 - val_acc: 0.8331\n",
      "Epoch 15/25\n",
      " - 399s - loss: 0.1186 - acc: 0.9569 - val_loss: 0.5432 - val_acc: 0.8396\n",
      "Epoch 16/25\n",
      " - 397s - loss: 0.1159 - acc: 0.9576 - val_loss: 0.5685 - val_acc: 0.8370\n",
      "Epoch 17/25\n",
      " - 398s - loss: 0.1113 - acc: 0.9610 - val_loss: 0.5459 - val_acc: 0.8492\n",
      "Epoch 18/25\n",
      " - 4618s - loss: 0.1527 - acc: 0.9428 - val_loss: 0.5188 - val_acc: 0.8422\n",
      "Epoch 19/25\n",
      " - 428s - loss: 0.1421 - acc: 0.9465 - val_loss: 0.5491 - val_acc: 0.8427\n",
      "Epoch 20/25\n",
      " - 1268s - loss: 0.1138 - acc: 0.9584 - val_loss: 0.5462 - val_acc: 0.8458\n",
      "Epoch 21/25\n",
      " - 2021s - loss: 0.1186 - acc: 0.9570 - val_loss: 0.5658 - val_acc: 0.8437\n",
      "Epoch 22/25\n",
      " - 2063s - loss: 0.1044 - acc: 0.9619 - val_loss: 0.5594 - val_acc: 0.8450\n",
      "Epoch 23/25\n",
      " - 2117s - loss: 0.0954 - acc: 0.9658 - val_loss: 0.5751 - val_acc: 0.8457\n",
      "Epoch 24/25\n",
      " - 2076s - loss: 0.0909 - acc: 0.9688 - val_loss: 0.6008 - val_acc: 0.8440\n",
      "Epoch 25/25\n",
      " - 2485s - loss: 0.0900 - acc: 0.9689 - val_loss: 0.5993 - val_acc: 0.8392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ad8e3ad080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=25, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 694s 28ms/step\n",
      "Test score: 0.5993069805335999\n",
      "Test accuracy: 0.839160000038147\n",
      "Accuracy: 83.92%\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "print(\"Accuracy: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time to train a GRU is less than LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
